{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cP2DCAVdt708"
   },
   "outputs": [],
   "source": [
    "import sionna\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "gpu_num = 0 # Use \"\" to use the CPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{gpu_num}\"\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Import Sionna\n",
    "try:\n",
    "    import sionna\n",
    "except ImportError as e:\n",
    "    # Install Sionna if package is not already installed\n",
    "    import os\n",
    "    os.system(\"pip install sionna\")\n",
    "    import sionna\n",
    "\n",
    "#from sionna.channel import AWGN\n",
    "from sionna.phy.channel import RayleighBlockFading\n",
    "from sionna.phy.utils import ebnodb2no, log10, expand_to_rank, insert_dims\n",
    "from sionna.phy.fec.ldpc import LDPC5GEncoder, LDPC5GDecoder\n",
    "from sionna.phy.mapping import BinarySource, Mapper, Demapper, Constellation\n",
    "from sionna.phy.channel import FlatFadingChannel, KroneckerModel\n",
    "from sionna.phy.utils import sim_ber\n",
    "# Configure the notebook to use only a single GPU and allocate only as much memory as needed\n",
    "# For more details, see https://www.tensorflow.org/guide/gpu\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "# Avoid warnings from TensorFlow\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer, Dense\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QCkW7VStuajI"
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "# SNR range for evaluation and training [dB]\n",
    "###############################################\n",
    "ebno_db_min = -2\n",
    "ebno_db_max = 4\n",
    "\n",
    "###############################################\n",
    "# Modulation and coding configuration\n",
    "###############################################\n",
    "num_bits_per_symbol = 6 # Baseline is 64-QAM\n",
    "modulation_order = 2**num_bits_per_symbol\n",
    "coderate = 0.5 # Coderate for the outer code\n",
    "n = 1500 # Codeword length [bit]. Must be a multiple of num_bits_per_symbol\n",
    "num_symbols_per_codeword = n//num_bits_per_symbol # Number of modulated baseband symbols per codeword\n",
    "k = int(n*coderate) # Number of information bits per codeword\n",
    "\n",
    "###############################################\n",
    "# Training configuration\n",
    "###############################################\n",
    "num_training_iterations_conventional = 1000 #10000 # Number of training iterations for conventional training\n",
    "# Number of training iterations with RL-based training for the alternating training phase and fine-tuning of the receiver phase\n",
    "num_training_iterations_rl_alt = 700 #7000\n",
    "num_training_iterations_rl_finetuning = 300 #3000\n",
    "###############################################\n",
    "# Meta-RL Training configuration\n",
    "###############################################\n",
    "num_training_iterations_meta_rl = 500 #1000 # Number of training iterations for Meta-RL training\n",
    "meta_batch_size = 8 # Number of tasks for each meta-training iteration\n",
    "\n",
    "training_batch_size = tf.constant(32, tf.int32) # Training batch size\n",
    "rl_perturbation_var = 0.01 # Variance of the perturbation used for RL-based training of the transmitter\n",
    "model_weights_path_conventional_training = \"awgn_autoencoder_weights_conventional_training\" # Filename to save the autoencoder weights once conventional training is done\n",
    "model_weights_path_rl_training = \"awgn_autoencoder_weights_rl_training\" # Filename to save the autoencoder weights once RL-based training is done\n",
    "model_weights_path_metarl_training = \"awgn_autoencoder_weights_metarl_training\" # Filename to save the autoencoder weights once RL-based training is done\n",
    "\n",
    "###############################################\n",
    "# Evaluation configuration\n",
    "###############################################\n",
    "results_filename = \"awgn_autoencoder_results\" # Location to save the results\n",
    "def save_weights(model, model_weights_path):\n",
    "    weights = model.get_weights()\n",
    "    with open(model_weights_path + \"__rf\", 'wb') as f:\n",
    "        pickle.dump(weights, f)\n",
    "\n",
    "# Utility function to load and set weights of a model\n",
    "def load_weights(model, model_weights_path):\n",
    "    model(1, tf.constant(10.0, tf.float32))\n",
    "    with open(model_weights_path + \"__rf\", 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "    model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sionna.phy.utils import ebnodb2no, compute_ser, compute_ber, PlotBER\n",
    "from sionna.phy.channel import FlatFadingChannel, KroneckerModel\n",
    "from sionna.phy.channel.utils import exp_corr_mat\n",
    "from sionna.phy.mimo import lmmse_equalizer\n",
    "from sionna.phy.mapping import BinarySource, QAMSource, SymbolDemapper, Mapper, Demapper\n",
    "from sionna.phy.fec.ldpc import LDPC5GEncoder, LDPC5GDecoder\n",
    "num_tx = 4\n",
    "num_rx = 16\n",
    "\n",
    "tf.config.xla_compat=True\n",
    "class Baseline(tf.keras.Model):\n",
    "    def __init__(self, spatial_corr=None):\n",
    "        super().__init__()\n",
    "        self.n = n \n",
    "        self.k = k  \n",
    "        self.coderate = coderate\n",
    "        self.num_bits_per_symbol = num_bits_per_symbol\n",
    "        self.num_tx_ant = num_tx\n",
    "        self.num_rx_ant = num_rx\n",
    "        self.binary_source = BinarySource()\n",
    "        self.encoder = LDPC5GEncoder(self.k, self.n)\n",
    "        self.mapper = Mapper(\"qam\", self.num_bits_per_symbol)\n",
    "        self.demapper = Demapper(\"app\", \"qam\", self.num_bits_per_symbol)\n",
    "        self.decoder = LDPC5GDecoder(self.encoder, hard_out=True)\n",
    "        self.channel = FlatFadingChannel(self.num_tx_ant,\n",
    "                                         self.num_rx_ant,\n",
    "                                         spatial_corr=spatial_corr,\n",
    "                                         add_awgn=True,\n",
    "                                         return_channel=True)\n",
    "        \n",
    "    @tf.function(jit_compile=True)\n",
    "    def call(self, batch_size, ebno_db):\n",
    "        b = self.binary_source([batch_size, self.num_tx_ant, self.k])\n",
    "        c = self.encoder(b)\n",
    "        \n",
    "        x = self.mapper(c)\n",
    "        shape = tf.shape(x)\n",
    "        x = tf.reshape(x, [-1, self.num_tx_ant])\n",
    "        \n",
    "        no = ebnodb2no(ebno_db, self.num_bits_per_symbol, self.coderate)\n",
    "        # print(ebno_db)\n",
    "        # print(no, no.shape)\n",
    "        no *= np.sqrt(self.num_rx_ant)\n",
    "\n",
    "        y, h = self.channel(x, no)\n",
    "        # print(no, no.shape)\n",
    "        # print(self.num_rx_ant)\n",
    "        s = tf.complex(no*tf.eye(self.num_rx_ant, self.num_rx_ant), 0.0)\n",
    "        \n",
    "        x_hat, no_eff = lmmse_equalizer(y, h, s)\n",
    "        \n",
    "        x_hat = tf.reshape(x_hat, shape)\n",
    "        no_eff = tf.reshape(no_eff, shape)\n",
    "        \n",
    "        llr = self.demapper(x_hat, no_eff)\n",
    "        b_hat = self.decoder(llr)\n",
    "        \n",
    "        return b,  b_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralDemapper(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Deeper network for MIMO-OFDM processing\n",
    "        self._dense_1 = Dense(256, 'relu')\n",
    "        self._dense_2 = Dense(256, 'relu')\n",
    "        self._dense_3 = Dense(128, 'relu')\n",
    "        self._dense_4 = Dense(num_bits_per_symbol, None)\n",
    "\n",
    "    @tf.function(jit_compile=True)\n",
    "    def call(self, y, no):\n",
    "        # Get shapes for dynamic handling\n",
    "        batch_size = tf.shape(y)[0]\n",
    "        total_symbols = tf.shape(y)[1]\n",
    "        \n",
    "        # Using log10 scale helps with the performance\n",
    "        no_db = log10(no)\n",
    "        \n",
    "        # Stack real part, imaginary part, and noise as features\n",
    "        z = tf.stack([tf.math.real(y),\n",
    "                     tf.math.imag(y),\n",
    "                     no_db], axis=2)\n",
    "        \n",
    "        # Process through neural network\n",
    "        h = self._dense_1(z)\n",
    "        h = self._dense_2(h)\n",
    "        h = self._dense_3(h)\n",
    "        llr = self._dense_4(h)\n",
    "        \n",
    "        return llr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7RHeSXqvX8i"
   },
   "outputs": [],
   "source": [
    "from sionna.phy.ofdm import ResourceGrid, ResourceGridMapper, RemoveNulledSubcarriers, OFDMModulator\n",
    "from sionna.phy.ofdm import OFDMDemodulator\n",
    "from sionna.phy.ofdm import LSChannelEstimator, RZFPrecoder, LMMSEEqualizer\n",
    "from sionna.phy.channel.tr38901 import AntennaArray, CDL\n",
    "from sionna.phy.channel import cir_to_ofdm_channel, time_lag_discrete_time_channel, cir_to_time_channel, subcarrier_frequencies, ApplyOFDMChannel, ApplyTimeChannel\n",
    "from sionna.phy.mimo import StreamManagement\n",
    "from sionna.phy.mapping import Mapper, BinarySource, Constellation\n",
    "from sionna.phy.fec.ldpc import LDPC5GEncoder, LDPC5GDecoder\n",
    "from sionna.phy.utils import ebnodb2no\n",
    "import tensorflow as tf\n",
    "\n",
    "class E2ESystemCDLTraining(tf.keras.Model):\n",
    "    def __init__(self, training, \n",
    "                 domain=\"freq\", \n",
    "                 direction=\"uplink\", \n",
    "                 cdl_model=\"A\",\n",
    "                 delay_spread=100e-9,\n",
    "                 perfect_csi=True,\n",
    "                 speed=0.0,\n",
    "                 cyclic_prefix_length=16,\n",
    "                 pilot_ofdm_symbol_indices=[2, 11]):\n",
    "        super().__init__()\n",
    "        self.training = training\n",
    "\n",
    "        # CDL Parameters\n",
    "        self._domain = domain\n",
    "        self._direction = direction\n",
    "        self._cdl_model = cdl_model\n",
    "        self._delay_spread = delay_spread\n",
    "        self._perfect_csi = perfect_csi\n",
    "        self._speed = speed\n",
    "        self._cyclic_prefix_length = cyclic_prefix_length\n",
    "        self._pilot_ofdm_symbol_indices = pilot_ofdm_symbol_indices\n",
    "\n",
    "        # Basic system parameters\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.coderate = coderate\n",
    "        self.num_bits_per_symbol = num_bits_per_symbol\n",
    "        \n",
    "        # System parameters from CDL model\n",
    "        self._carrier_frequency = 2.6e9\n",
    "        self._subcarrier_spacing = 15e3\n",
    "        self._fft_size = 72\n",
    "        self._num_ofdm_symbols = 14\n",
    "        self._num_ut_ant = 4  # Must be a multiple of two as dual-polarized antennas are used\n",
    "        self._num_bs_ant = 8  # Must be a multiple of two as dual-polarized antennas are used\n",
    "        self._num_streams_per_tx = self._num_ut_ant\n",
    "        self._dc_null = True\n",
    "        self._num_guard_carriers = [5, 6]\n",
    "        self._pilot_pattern = \"kronecker\"\n",
    "        \n",
    "        # Set up components\n",
    "        self.binary_source = BinarySource()\n",
    "        if not self.training:\n",
    "            self.encoder = LDPC5GEncoder(k, self.n)\n",
    "            \n",
    "        # Trainable constellation for transmitter\n",
    "        constellation = Constellation(\"qam\", num_bits_per_symbol, trainable=True)\n",
    "        self.constellation = constellation\n",
    "        self.mapper = Mapper(constellation=constellation)\n",
    "        \n",
    "        # Stream management\n",
    "        self._sm = StreamManagement(np.array([[1]]), self._num_streams_per_tx)\n",
    "        \n",
    "        # Set up resource grid\n",
    "        self._rg = ResourceGrid(num_ofdm_symbols=self._num_ofdm_symbols,\n",
    "                            fft_size=self._fft_size,\n",
    "                            subcarrier_spacing=self._subcarrier_spacing,\n",
    "                            num_tx=1,\n",
    "                            num_streams_per_tx=self._num_streams_per_tx,\n",
    "                            cyclic_prefix_length=self._cyclic_prefix_length,\n",
    "                            num_guard_carriers=self._num_guard_carriers,\n",
    "                            dc_null=self._dc_null,\n",
    "                            pilot_pattern=self._pilot_pattern,\n",
    "                            pilot_ofdm_symbol_indices=self._pilot_ofdm_symbol_indices)\n",
    "        \n",
    "        # Then calculate bit counts based on resource grid\n",
    "        self.n = int(self._rg.num_data_symbols * num_bits_per_symbol)\n",
    "        self.k = int(self.n * coderate)\n",
    "        # Resource grid mapper\n",
    "        self._rg_mapper = ResourceGridMapper(self._rg)\n",
    "        \n",
    "        # Antenna arrays\n",
    "        self._ut_array = AntennaArray(num_rows=1,\n",
    "                                      num_cols=int(self._num_ut_ant/2),\n",
    "                                      polarization=\"dual\",\n",
    "                                      polarization_type=\"cross\",\n",
    "                                      antenna_pattern=\"38.901\",\n",
    "                                      carrier_frequency=self._carrier_frequency)\n",
    "\n",
    "        self._bs_array = AntennaArray(num_rows=1,\n",
    "                                      num_cols=int(self._num_bs_ant/2),\n",
    "                                      polarization=\"dual\",\n",
    "                                      polarization_type=\"cross\",\n",
    "                                      antenna_pattern=\"38.901\",\n",
    "                                      carrier_frequency=self._carrier_frequency)\n",
    "                                      \n",
    "        # Channel model\n",
    "        self._cdl = CDL(model=self._cdl_model,\n",
    "                        delay_spread=self._delay_spread,\n",
    "                        carrier_frequency=self._carrier_frequency,\n",
    "                        ut_array=self._ut_array,\n",
    "                        bs_array=self._bs_array,\n",
    "                        direction=self._direction,\n",
    "                        min_speed=self._speed)\n",
    "        \n",
    "        # Frequency domain computations\n",
    "        self._frequencies = subcarrier_frequencies(self._rg.fft_size, self._rg.subcarrier_spacing)\n",
    "        \n",
    "        # Channel application components\n",
    "        if self._domain == \"freq\":\n",
    "            self._channel_freq = ApplyOFDMChannel(add_awgn=True)\n",
    "        elif self._domain == \"time\":\n",
    "            self._l_min, self._l_max = time_lag_discrete_time_channel(self._rg.bandwidth)\n",
    "            self._l_tot = self._l_max - self._l_min + 1\n",
    "            self._channel_time = ApplyTimeChannel(self._rg.num_time_samples,\n",
    "                                                  l_tot=self._l_tot,\n",
    "                                                  add_awgn=True)\n",
    "            self._modulator = OFDMModulator(self._cyclic_prefix_length)\n",
    "            self._demodulator = OFDMDemodulator(self._fft_size, self._l_min, self._cyclic_prefix_length)\n",
    "        \n",
    "        # Receiver components\n",
    "        if self._direction == \"downlink\":\n",
    "            self._zf_precoder = RZFPrecoder(self._rg, self._sm, return_effective_channel=True)\n",
    "            \n",
    "        # Channel estimation\n",
    "        self._ls_est = LSChannelEstimator(self._rg, interpolation_type=\"nn\")\n",
    "        self._lmmse_equ = LMMSEEqualizer(self._rg, self._sm)\n",
    "        self._remove_nulled_scs = RemoveNulledSubcarriers(self._rg)\n",
    "        \n",
    "        # Replace standard demapper with our neural demapper\n",
    "        self.demapper = NeuralDemapper()\n",
    "        \n",
    "        # Add decoder if not in training mode\n",
    "        if not self.training:\n",
    "            self.decoder = LDPC5GDecoder(self.encoder, hard_out=True)\n",
    "            \n",
    "        # Loss function\n",
    "        if self.training:\n",
    "            self.bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, batch_size, ebno_db):\n",
    "        # Generate bits\n",
    "        if self.training:\n",
    "            c = self.binary_source([batch_size, 1, self._num_streams_per_tx, self.n])\n",
    "        else:\n",
    "            b = self.binary_source([batch_size, 1, self._num_streams_per_tx, self.k])\n",
    "            c = self.encoder(b)\n",
    "        \n",
    "        # Map bits to symbols\n",
    "        x = self.mapper(c)\n",
    "        x_rg = self._rg_mapper(x)\n",
    "        \n",
    "        # Calculate noise power\n",
    "        no = ebnodb2no(ebno_db, self.num_bits_per_symbol, self.coderate, self._rg)\n",
    "        \n",
    "        # Apply channel - time or frequency domain\n",
    "        if self._domain == \"time\":\n",
    "            # Time-domain simulations\n",
    "            a, tau = self._cdl(batch_size, self._rg.num_time_samples+self._l_tot-1, self._rg.bandwidth)\n",
    "            h_time = cir_to_time_channel(self._rg.bandwidth, a, tau,\n",
    "                                        l_min=self._l_min, l_max=self._l_max, normalize=True)\n",
    "\n",
    "            # Downsample path gains for frequency domain\n",
    "            a_freq = a[...,self._rg.cyclic_prefix_length:-1:(self._rg.fft_size+self._rg.cyclic_prefix_length)]\n",
    "            a_freq = a_freq[...,:self._rg.num_ofdm_symbols]\n",
    "            h_freq = cir_to_ofdm_channel(self._frequencies, a_freq, tau, normalize=True)\n",
    "\n",
    "            if self._direction == \"downlink\":\n",
    "                x_rg, g = self._zf_precoder(x_rg, h_freq)\n",
    "\n",
    "            x_time = self._modulator(x_rg)\n",
    "            y_time = self._channel_time(x_time, h_time, no)\n",
    "            y = self._demodulator(y_time)\n",
    "\n",
    "        else:  # Frequency domain\n",
    "            cir = self._cdl(batch_size, self._rg.num_ofdm_symbols, 1/self._rg.ofdm_symbol_duration)\n",
    "            h_freq = cir_to_ofdm_channel(self._frequencies, *cir, normalize=True)\n",
    "\n",
    "            if self._direction == \"downlink\":\n",
    "                x_rg, g = self._zf_precoder(x_rg, h_freq)\n",
    "\n",
    "            y = self._channel_freq(x_rg, h_freq, no)\n",
    "        \n",
    "        # Channel estimation\n",
    "        if self._perfect_csi:\n",
    "            if self._direction == \"uplink\":\n",
    "                h_hat = self._remove_nulled_scs(h_freq)\n",
    "            elif self._direction == \"downlink\":\n",
    "                h_hat = g\n",
    "            err_var = 0.0\n",
    "        else:\n",
    "            h_hat, err_var = self._ls_est(y, no)\n",
    "        \n",
    "        # Equalize\n",
    "        x_hat, no_eff = self._lmmse_equ(y, h_hat, err_var, no)\n",
    "        print(f\"Batch Size: {batch_size}, x_hat shape: {x_hat.shape}, no_eff shape: {no_eff.shape}\")\n",
    "        x_hat_reshaped = tf.reshape(x_hat, [batch_size, -1])  # Flatten all dimensions after batch\n",
    "        no_eff_reshaped = tf.reshape(no_eff, [batch_size, -1])\n",
    "\n",
    "        # Use neural demapper\n",
    "        llr = self.demapper(x_hat_reshaped, no_eff_reshaped)\n",
    "        # Reshape LLR outputs to match expected format for loss calculation or decoding\n",
    "        if self.training:\n",
    "\n",
    "            llr = tf.reshape(llr, tf.shape(c))\n",
    "            loss = self.bce(c, llr)\n",
    "            return loss\n",
    "        else:\n",
    "            llr = tf.reshape(llr, [batch_size, 1, self._num_streams_per_tx, -1])\n",
    "            b_hat = self.decoder(llr)\n",
    "            return b, b_hat\n",
    "\n",
    "def conventional_training(model):\n",
    "    # Optimizer used to apply gradients\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step():\n",
    "        # Sampling a batch of SNRs\n",
    "        ebno_db = tf.random.uniform(shape=[training_batch_size], minval=ebno_db_min, maxval=ebno_db_max)\n",
    "        # Forward pass\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = model(training_batch_size, ebno_db)\n",
    "        # Computing and applying gradients\n",
    "        weights = model.trainable_variables\n",
    "        grads = tape.gradient(loss, weights)\n",
    "        optimizer.apply_gradients(zip(grads, weights))\n",
    "        return loss\n",
    "\n",
    "    for i in range(num_training_iterations_conventional):\n",
    "        loss = train_step()\n",
    "        # Printing periodically the progress\n",
    "        if i % 100 == 0:\n",
    "            print('Iteration {}/{}  BCE: {:.4f}'.format(i, num_training_iterations_conventional, loss.numpy()), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: Tensor(\"batch_size:0\", shape=(), dtype=int32), x_hat shape: (32, 1, 4, 720), no_eff shape: (32, 1, 4, 720)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Projects\\Unfinished\\Sionna-CDL-Channel\\.env\\Lib\\site-packages\\keras\\src\\layers\\layer.py:393: UserWarning: `build()` was called on layer 'neural_demapper_21', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: Tensor(\"batch_size:0\", shape=(), dtype=int32), x_hat shape: (32, 1, 4, 720), no_eff shape: (32, 1, 4, 720)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Ahmad\\AppData\\Local\\Temp\\ipykernel_9808\\3536717681.py\", line 226, in train_step  *\n        loss = model(training_batch_size, ebno_db)\n    File \"e:\\Projects\\Unfinished\\Sionna-CDL-Channel\\.env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Ahmad\\AppData\\Local\\Temp\\__autograph_generated_file3vomspah.py\", line 188, in tf__call\n        ag__.if_stmt(ag__.ld(self).training, if_body_7, else_body_7, get_state_7, set_state_7, ('do_return', 'retval_', 'llr'), 2)\n    File \"C:\\Users\\Ahmad\\AppData\\Local\\Temp\\__autograph_generated_file3vomspah.py\", line 167, in if_body_7\n        llr = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(llr), ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(c),), None, fscope)), None, fscope)\n\n    ValueError: Exception encountered when calling E2ESystemCDLTraining.call().\n    \n    \u001b[1min user code:\n    \n        File \"C:\\Users\\Ahmad\\AppData\\Local\\Temp\\ipykernel_9808\\4048474048.py\", line 208, in call  *\n            llr = tf.reshape(llr, tf.shape(c))\n    \n        ValueError: Tried to convert 'tensor' to a tensor and failed. Error: None values not supported.\n    \u001b[0m\n    \n    Arguments received by E2ESystemCDLTraining.call():\n      • batch_size=tf.Tensor(shape=(), dtype=int32)\n      • ebno_db=tf.Tensor(shape=(32,), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Instantiate and train the end-to-end system with CDL channel\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m E2ESystemCDLTraining(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mconventional_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Save weights\u001b[39;00m\n\u001b[0;32m      7\u001b[0m save_weights(model, model_weights_path_conventional_training)\n",
      "Cell \u001b[1;32mIn[58], line 234\u001b[0m, in \u001b[0;36mconventional_training\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_training_iterations_conventional):\n\u001b[1;32m--> 234\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# Printing periodically the progress\u001b[39;00m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32me:\\Projects\\Unfinished\\Sionna-CDL-Channel\\.env\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file8f7u2knr.py:14\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m ebno_db \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform, (), \u001b[38;5;28mdict\u001b[39m(shape\u001b[38;5;241m=\u001b[39m[ag__\u001b[38;5;241m.\u001b[39mld(training_batch_size)], minval\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(ebno_db_min), maxval\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(ebno_db_max)), fscope)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_batch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mebno_db\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m weights \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39mtrainable_variables\n\u001b[0;32m     16\u001b[0m grads \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(loss), ag__\u001b[38;5;241m.\u001b[39mld(weights)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32me:\\Projects\\Unfinished\\Sionna-CDL-Channel\\.env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file3vomspah.py:188\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, batch_size, ebno_db)\u001b[0m\n\u001b[0;32m    186\u001b[0m b_hat \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb_hat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    187\u001b[0m loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 188\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body_7\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body_7\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_7\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_7\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdo_return\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mretval_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mllr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fscope\u001b[38;5;241m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file3vomspah.py:167\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.if_body_7\u001b[1;34m()\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mif_body_7\u001b[39m():\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m do_return, retval_, llr\n\u001b[1;32m--> 167\u001b[0m     llr \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mbce, (ag__\u001b[38;5;241m.\u001b[39mld(c), ag__\u001b[38;5;241m.\u001b[39mld(llr)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Ahmad\\AppData\\Local\\Temp\\ipykernel_9808\\3536717681.py\", line 226, in train_step  *\n        loss = model(training_batch_size, ebno_db)\n    File \"e:\\Projects\\Unfinished\\Sionna-CDL-Channel\\.env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Ahmad\\AppData\\Local\\Temp\\__autograph_generated_file3vomspah.py\", line 188, in tf__call\n        ag__.if_stmt(ag__.ld(self).training, if_body_7, else_body_7, get_state_7, set_state_7, ('do_return', 'retval_', 'llr'), 2)\n    File \"C:\\Users\\Ahmad\\AppData\\Local\\Temp\\__autograph_generated_file3vomspah.py\", line 167, in if_body_7\n        llr = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(llr), ag__.converted_call(ag__.ld(tf).shape, (ag__.ld(c),), None, fscope)), None, fscope)\n\n    ValueError: Exception encountered when calling E2ESystemCDLTraining.call().\n    \n    \u001b[1min user code:\n    \n        File \"C:\\Users\\Ahmad\\AppData\\Local\\Temp\\ipykernel_9808\\4048474048.py\", line 208, in call  *\n            llr = tf.reshape(llr, tf.shape(c))\n    \n        ValueError: Tried to convert 'tensor' to a tensor and failed. Error: None values not supported.\n    \u001b[0m\n    \n    Arguments received by E2ESystemCDLTraining.call():\n      • batch_size=tf.Tensor(shape=(), dtype=int32)\n      • ebno_db=tf.Tensor(shape=(32,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Fix the seed for reproducible trainings\n",
    "tf.random.set_seed(1)\n",
    "# Instantiate and train the end-to-end system with CDL channel\n",
    "model = E2ESystemCDLTraining(training=True)\n",
    "conventional_training(model)\n",
    "# Save weights\n",
    "save_weights(model, model_weights_path_conventional_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vWiEiaxAaA_Q"
   },
   "outputs": [],
   "source": [
    "class E2ESystemRLTraining(tf.keras.Model):\n",
    "    def __init__(self, training, spatial_corr=None):\n",
    "        super().__init__()\n",
    "        self.training = training\n",
    "\n",
    "        self.n = n \n",
    "        self.k = k  \n",
    "        self.coderate = coderate\n",
    "        self.num_bits_per_symbol = num_bits_per_symbol\n",
    "        self.num_tx_ant = num_tx\n",
    "        self.num_rx_ant = num_rx\n",
    "        self.binary_source = BinarySource()\n",
    "        if not self.training:\n",
    "            self.encoder = LDPC5GEncoder(k, n, num_bits_per_symbol) \n",
    "        # Trainable constellation\n",
    "        constellation = Constellation(\"qam\", num_bits_per_symbol, trainable=True)\n",
    "        self.constellation = constellation\n",
    "        self.mapper = Mapper(constellation=constellation)\n",
    "        self.channel = FlatFadingChannel(self.num_tx_ant,\n",
    "                                         self.num_rx_ant,\n",
    "                                         spatial_corr=spatial_corr,\n",
    "                                         add_awgn=True,\n",
    "                                         return_channel=True)\n",
    "        self.demapper = NeuralDemapper()\n",
    "        # To reduce the computational complexity of training, the outer code is not used when training,\n",
    "        # as it is not required\n",
    "        if not self.training:\n",
    "            self.decoder = LDPC5GDecoder(self.encoder, hard_out=True)\n",
    "            \n",
    "        #################\n",
    "        # Loss function\n",
    "        #################\n",
    "        if self.training:\n",
    "            self.bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "     \n",
    "    def call(self, batch_size, ebno_db, perturbation_variance=tf.constant(0.0, tf.float32)):\n",
    "        if self.training:\n",
    "            c = self.binary_source([batch_size, self.num_tx_ant, self.n])\n",
    "        else:\n",
    "            b = self.binary_source([batch_size, self.num_tx_ant, self.k])\n",
    "            c = self.encoder(b)\n",
    "            \n",
    "        x = self.mapper(c)\n",
    "        shape = tf.shape(x)\n",
    "        x = tf.reshape(x, [-1, self.num_tx_ant])\n",
    "        \n",
    "        epsilon_r = tf.random.normal(tf.shape(x))*tf.sqrt(0.5*perturbation_variance)\n",
    "        epsilon_i = tf.random.normal(tf.shape(x))*tf.sqrt(0.5*perturbation_variance)\n",
    "        epsilon = tf.complex(epsilon_r, epsilon_i) # [batch size, num_symbols_per_codeword]\n",
    "        x_p = x + epsilon\n",
    "\n",
    "        \n",
    "        no = ebnodb2no(ebno_db, self.num_bits_per_symbol, self.coderate)\n",
    "        no *= np.sqrt(self.num_rx_ant)\n",
    "        \n",
    "        y, h = self.channel([x_p, no])\n",
    "        s = tf.complex(no*tf.eye(self.num_rx_ant, self.num_rx_ant), 0.0)\n",
    "        \n",
    "        x_hat, no_eff = lmmse_equalizer(y, h, s)\n",
    "\n",
    "        x_hat = tf.reshape(x_hat, shape)\n",
    "        no_eff = tf.reshape(no_eff, shape)\n",
    "\n",
    "        llr = self.demapper([x_hat, no_eff])\n",
    "        # If training, outer decoding is not performed and the BCE is returned\n",
    "        if self.training:\n",
    "            c = tf.reshape(c, [-1, num_symbols_per_codeword * self.num_tx_ant, num_bits_per_symbol])\n",
    "            llr = tf.reshape(llr, [-1, num_symbols_per_codeword * self.num_tx_ant, num_bits_per_symbol])\n",
    "            bce = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(c, llr), axis=2) # Avergare over the bits mapped to a same baseband symbol\n",
    "            # The RX loss is the usual average BCE\n",
    "            rx_loss = tf.reduce_mean(bce)\n",
    "            # From the TX side, the BCE is seen as a feedback from the RX through which backpropagation is not possible\n",
    "            bce = tf.stop_gradient(bce) # [batch size, num_symbols_per_codeword]\n",
    "            x_p = tf.stop_gradient(x_p)\n",
    "            p = x_p-x # [batch size, num_symbols_per_codeword] Gradient is backpropagated through `x`\n",
    "            p = tf.reshape(p, bce.shape)\n",
    "            tx_loss = tf.square(tf.math.real(p)) + tf.square(tf.math.imag(p)) # [batch size, num_symbols_per_codeword]\n",
    "            tx_loss = -bce*tx_loss/rl_perturbation_var # [batch size, num_symbols_per_codeword]\n",
    "            tx_loss = tf.reduce_mean(tx_loss)\n",
    "            return tx_loss, rx_loss\n",
    "        else:\n",
    "            # Outer decoding\n",
    "            llr = tf.reshape(llr, [batch_size, self.num_tx_ant, self.n])\n",
    "            b_hat = self.decoder(llr)\n",
    "            return b, b_hat # Ground truth and reconstructed information bits returned for BER/BLER computation\n",
    "\n",
    "def rl_based_training(model):\n",
    "    # Optimizers used to apply gradients\n",
    "    optimizer_tx = tf.keras.optimizers.Adam() # For training the transmitter\n",
    "    optimizer_rx = tf.keras.optimizers.Adam() # For training the receiver\n",
    "\n",
    "    # Function that implements one transmitter training iteration using RL.\n",
    "    @tf.function\n",
    "    def train_tx():\n",
    "        # Sampling a batch of SNRs\n",
    "        ebno_db = tf.random.uniform(shape=[4], minval=ebno_db_min, maxval=ebno_db_max)\n",
    "        # Forward pass\n",
    "        for no in ebno_db:\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Keep only the TX loss\n",
    "                tx_loss, _ = model(4, no,\n",
    "                                   tf.constant(rl_perturbation_var, tf.float32)) # Perturbation are added to enable RL exploration\n",
    "            ## Computing and applying gradients\n",
    "            weights = model.trainable_weights\n",
    "            grads = tape.gradient(tx_loss, weights)\n",
    "            optimizer_tx.apply_gradients(zip(grads, weights))\n",
    "    \n",
    "    # Function that implements one receiver training iteration\n",
    "    @tf.function\n",
    "    def train_rx():\n",
    "        # Sampling a batch of SNRs\n",
    "        ebno_db = tf.random.uniform(shape=[4], minval=ebno_db_min, maxval=ebno_db_max)\n",
    "        # Forward pass\n",
    "        for no in ebno_db:\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Keep only the RX loss\n",
    "                _, rx_loss = model(4, no) # No perturbation is added\n",
    "            ## Computing and applying gradients\n",
    "            weights = model.trainable_weights\n",
    "            grads = tape.gradient(rx_loss, weights)\n",
    "            optimizer_rx.apply_gradients(zip(grads, weights))\n",
    "    \n",
    "    # Training loop.\n",
    "    for i in tqdm(pbar := tqdm(range(num_training_iterations_rl_alt))):\n",
    "        # 10 steps of receiver training are performed to keep it ahead of the transmitter\n",
    "        # as it is used for computing the losses when training the transmitter\n",
    "        for _ in range(10):\n",
    "            train_rx()\n",
    "        # One step of transmitter training\n",
    "        # train_tx()     \n",
    "        # get progress training\n",
    "        ebno_db = tf.random.uniform(shape=[3], minval=ebno_db_min, maxval=ebno_db_max)\n",
    "        with tf.GradientTape() as tape:\n",
    "            _, rx_loss = model(4, ebno_db[1])\n",
    "        # Printing periodically the progress\n",
    "        txt = f\"BCE: {rx_loss.numpy()}\"\n",
    "        pbar.set_description(txt)\n",
    "    \n",
    "    # Once alternating training is done, the receiver is fine-tuned.\n",
    "    print('Receiver fine-tuning... ')\n",
    "    for i in tqdm(pbar := tqdm(range(num_training_iterations_rl_finetuning))):\n",
    "        train_rx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_tx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Instantiate and train the end-to-end system\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mE2ESystemRLTraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m rl_based_training(model)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Save weights\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m, in \u001b[0;36mE2ESystemRLTraining.__init__\u001b[1;34m(self, training, spatial_corr)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoderate \u001b[38;5;241m=\u001b[39m coderate\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_bits_per_symbol \u001b[38;5;241m=\u001b[39m num_bits_per_symbol\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_tx_ant \u001b[38;5;241m=\u001b[39m \u001b[43mnum_tx\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_rx_ant \u001b[38;5;241m=\u001b[39m num_rx\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary_source \u001b[38;5;241m=\u001b[39m BinarySource()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_tx' is not defined"
     ]
    }
   ],
   "source": [
    "# Fix the seed for reproducible trainings\n",
    "tf.random.set_seed(1)\n",
    "# Instantiate and train the end-to-end system\n",
    "model = E2ESystemRLTraining(training=True)\n",
    "rl_based_training(model)\n",
    "# Save weights\n",
    "save_weights(model, model_weights_path_rl_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_tx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 188\u001b[0m\n\u001b[0;32m    186\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# init model\u001b[39;00m\n\u001b[1;32m--> 188\u001b[0m meta_model \u001b[38;5;241m=\u001b[39m \u001b[43mE2ESystemMetaRLTraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m meta_model(\u001b[38;5;241m1\u001b[39m, tf\u001b[38;5;241m.\u001b[39mVariable(\u001b[38;5;241m5.0\u001b[39m))\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# load pre-trained model before -- you can use this to continously train the model if the memory capacity is not enough # Previous total iterations = 1k\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# load_weights(meta_model, model_weights_path_metarl_training) \u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# train meta-model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m, in \u001b[0;36mE2ESystemMetaRLTraining.__init__\u001b[1;34m(self, training, spatial_corr)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoderate \u001b[38;5;241m=\u001b[39m coderate\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_bits_per_symbol \u001b[38;5;241m=\u001b[39m num_bits_per_symbol\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_tx_ant \u001b[38;5;241m=\u001b[39m \u001b[43mnum_tx\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_rx_ant \u001b[38;5;241m=\u001b[39m num_rx\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary_source \u001b[38;5;241m=\u001b[39m BinarySource()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_tx' is not defined"
     ]
    }
   ],
   "source": [
    "class E2ESystemMetaRLTraining(tf.keras.Model):\n",
    "    def __init__(self, training, spatial_corr=None):\n",
    "        super().__init__()\n",
    "        self.training = training\n",
    "\n",
    "        self.n = n \n",
    "        self.k = k  \n",
    "        self.coderate = coderate\n",
    "        self.num_bits_per_symbol = num_bits_per_symbol\n",
    "        self.num_tx_ant = num_tx\n",
    "        self.num_rx_ant = num_rx\n",
    "        self.binary_source = BinarySource()\n",
    "        if not self.training:\n",
    "            self.encoder = LDPC5GEncoder(k, n, num_bits_per_symbol) \n",
    "        # Trainable constellation\n",
    "        constellation = Constellation(\"qam\", num_bits_per_symbol, trainable=True)\n",
    "        self.constellation = constellation\n",
    "        self.mapper = Mapper(constellation=constellation)\n",
    "        self.channel = FlatFadingChannel(self.num_tx_ant,\n",
    "                                         self.num_rx_ant,\n",
    "                                         spatial_corr=spatial_corr,\n",
    "                                         add_awgn=True,\n",
    "                                         return_channel=True)\n",
    "        self.demapper = NeuralDemapper()\n",
    "        # To reduce the computational complexity of training, the outer code is not used when training,\n",
    "        # as it is not required\n",
    "        if not self.training:\n",
    "            self.decoder = LDPC5GDecoder(self.encoder, hard_out=True)\n",
    "            \n",
    "        #################\n",
    "        # Loss function\n",
    "        #################\n",
    "        if self.training:\n",
    "            self.bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "     \n",
    "    def call(self, batch_size, ebno_db, perturbation_variance=tf.constant(0.0, tf.float32)):\n",
    "        if self.training:\n",
    "            c = self.binary_source([batch_size, self.num_tx_ant, self.n])\n",
    "        else:\n",
    "            b = self.binary_source([batch_size, self.num_tx_ant, self.k])\n",
    "            c = self.encoder(b)\n",
    "            \n",
    "        x = self.mapper(c)\n",
    "        shape = tf.shape(x)\n",
    "        x = tf.reshape(x, [-1, self.num_tx_ant])\n",
    "        \n",
    "        epsilon_r = tf.random.normal(tf.shape(x))*tf.sqrt(0.5*perturbation_variance)\n",
    "        epsilon_i = tf.random.normal(tf.shape(x))*tf.sqrt(0.5*perturbation_variance)\n",
    "        epsilon = tf.complex(epsilon_r, epsilon_i) # [batch size, num_symbols_per_codeword]\n",
    "        x_p = x + epsilon\n",
    "\n",
    "        \n",
    "        no = ebnodb2no(ebno_db, self.num_bits_per_symbol, self.coderate)\n",
    "        no *= np.sqrt(self.num_rx_ant)\n",
    "        \n",
    "        y, h = self.channel([x_p, no])\n",
    "        s = tf.complex(no*tf.eye(self.num_rx_ant, self.num_rx_ant), 0.0)\n",
    "        \n",
    "        x_hat, no_eff = lmmse_equalizer(y, h, s)\n",
    "\n",
    "        x_hat = tf.reshape(x_hat, shape)\n",
    "        no_eff = tf.reshape(no_eff, shape)\n",
    "\n",
    "        llr = self.demapper([x_hat, no_eff])\n",
    "        # If training, outer decoding is not performed and the BCE is returned\n",
    "        if self.training:\n",
    "            c = tf.reshape(c, [-1, num_symbols_per_codeword * self.num_tx_ant, num_bits_per_symbol])\n",
    "            llr = tf.reshape(llr, [-1, num_symbols_per_codeword * self.num_tx_ant, num_bits_per_symbol])\n",
    "            bce = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(c, llr), axis=2) # Avergare over the bits mapped to a same baseband symbol\n",
    "            # The RX loss is the usual average BCE\n",
    "            rx_loss = tf.reduce_mean(bce)\n",
    "            # From the TX side, the BCE is seen as a feedback from the RX through which backpropagation is not possible\n",
    "            bce = tf.stop_gradient(bce) # [batch size, num_symbols_per_codeword]\n",
    "            x_p = tf.stop_gradient(x_p)\n",
    "            p = x_p-x # [batch size, num_symbols_per_codeword] Gradient is backpropagated through `x`\n",
    "            p = tf.reshape(p, bce.shape)\n",
    "            tx_loss = tf.square(tf.math.real(p)) + tf.square(tf.math.imag(p)) # [batch size, num_symbols_per_codeword]\n",
    "            tx_loss = -bce*tx_loss/rl_perturbation_var # [batch size, num_symbols_per_codeword]\n",
    "            tx_loss = tf.reduce_mean(tx_loss)\n",
    "            return tx_loss, rx_loss\n",
    "        else:\n",
    "            # Outer decoding\n",
    "            llr = tf.reshape(llr, [batch_size, self.num_tx_ant, self.n])\n",
    "            b_hat = self.decoder(llr)\n",
    "            return b, b_hat # Ground truth and reconstructed information bits returned for BER/BLER computation\n",
    "\n",
    "\n",
    "def copy_model(model):\n",
    "    adapted_model = tf.keras.models.clone_model(model)\n",
    "    adapted_model(1, tf.constant(10.0, tf.float32))\n",
    "    adapted_model.set_weights(model.get_weights())\n",
    "    return adapted_model\n",
    "\n",
    "def inner_train(model, task_snr_values, inner_interations = 3):\n",
    "    # return grads & avg loss\n",
    "    def get_avg_grads(grads):\n",
    "        _grads = []\n",
    "        for j in range(len(grads[0])):\n",
    "            __grads = []\n",
    "            for x in grads:\n",
    "                __grads.append(x[j])\n",
    "            __grads = tf.math.reduce_mean(__grads, axis=0)\n",
    "            _grads.append(\n",
    "                tf.Variable(__grads)\n",
    "            )\n",
    "        return _grads\n",
    "            \n",
    "    inner_loss = []\n",
    "    tx_grad = []\n",
    "    rx_grad = []\n",
    "    \n",
    "    adapted_model = copy_model(model)\n",
    "    for _ in range(inner_interations): \n",
    "        for snr in task_snr_values:\n",
    "            with tf.GradientTape() as tx_tape:\n",
    "                _, rx_loss = adapted_model(4, snr)\n",
    "            weights = adapted_model.demapper.trainable_variables\n",
    "            grads = tx_tape.gradient(rx_loss, weights)\n",
    "            rx_grad.append(grads)    \n",
    "        \n",
    "            inner_loss.append(rx_loss.numpy())\n",
    "            \n",
    "            with tf.GradientTape() as rx_tape:\n",
    "                tx_loss, _ = adapted_model(4, snr, tf.constant(rl_perturbation_var))\n",
    "            weights = adapted_model.constellation.trainable_variables\n",
    "            grads = rx_tape.gradient(tx_loss, weights)\n",
    "            tx_grad.append(grads)    \n",
    "        \n",
    "    return np.mean(inner_loss), get_avg_grads(tx_grad), get_avg_grads(rx_grad) \n",
    "\n",
    "def meta_rl_training(model):\n",
    "    lr_outer = 0.01\n",
    "    # Meta-training loop\n",
    "    num_training_iterations_meta_rl = 1000\n",
    "    for i in tqdm(pbar := tqdm(range(num_training_iterations_meta_rl))):\n",
    "        optimizer_constellation = tf.keras.optimizers.Adam()\n",
    "        optimizer_demapper = tf.keras.optimizers.Adam()\n",
    "        fixed_snr = ebno_db_max\n",
    "        task_snr_values = tf.random.uniform([8], minval=ebno_db_min, maxval=ebno_db_max)  \n",
    "        \n",
    "        inner_loss, tx_grad, rx_grads = inner_train(model, task_snr_values)\n",
    "        weights = model.demapper.trainable_variables  \n",
    "        optimizer_demapper.apply_gradients(zip(rx_grads, weights))\n",
    "        # weights = model.constellation.trainable_variables  \n",
    "        # optimizer_constellation.apply_gradients(zip(tx_grad, weights))\n",
    "        \n",
    "        # manually update, due to not able to use the optimizer directly\n",
    "        # model.constellation.set_weights(\n",
    "        #     tf.subtract(model.constellation.weights, tf.multiply(lr_outer, tx_grad[0])),\n",
    "        # )\n",
    "        # k = 0\n",
    "        # for j in range(len(model.demapper.layers)):\n",
    "        #     model.demapper.layers[j].set_weights([\n",
    "        #         tf.subtract(model.demapper.layers[j].kernel, tf.multiply(lr_outer, rx_grads[k])),\n",
    "        #         tf.subtract(model.demapper.layers[j].bias, tf.multiply(lr_outer, rx_grads[k+1])),\n",
    "        #     ])\n",
    "        #     k += 2\n",
    "            \n",
    "        # for the outerloop, train with variety of SNRs, but very few data\n",
    "        task_snr_values = tf.random.uniform([4], minval=ebno_db_max, maxval=ebno_db_max) \n",
    "        for snr in task_snr_values:\n",
    "            # with tf.GradientTape() as tx_tape:\n",
    "            #     tx_loss, _ = model(4, snr, tf.constant(rl_perturbation_var))\n",
    "            # weights = model.constellation.trainable_variables  \n",
    "            # grads = tx_tape.gradient(tx_loss, weights)\n",
    "            # optimizer_constellation.apply_gradients(zip(grads, weights))  \n",
    "    \n",
    "            with tf.GradientTape() as rx_tape:\n",
    "                _, rx_loss = model(4, snr)\n",
    "            weights = model.demapper.trainable_variables  \n",
    "            grads = rx_tape.gradient(rx_loss, weights)\n",
    "            optimizer_demapper.apply_gradients(zip(grads, weights))\n",
    "        \n",
    "        # txt = f\"Inner BCE: {inner_loss} || Tx BCE: {tx_loss.numpy()} - Rx BCE: {rx_loss.numpy()}\"\n",
    "        txt = f\"Avg BCE: {inner_loss}\"\n",
    "\n",
    "        pbar.set_description(txt)\n",
    "        \n",
    "        if (i%50)==0:\n",
    "            print(f\"Epoch {i}:\\t\" + txt)\n",
    "            save_weights(meta_model, model_weights_path_metarl_training)\n",
    "        # break\n",
    "    print(\"Meta-RL Training complete.\")\n",
    "\n",
    "# Now, use the modified code in place of the original RL-based training\n",
    "tf.random.set_seed(1)\n",
    "# init model\n",
    "meta_model = E2ESystemMetaRLTraining(training=True)\n",
    "meta_model(1, tf.Variable(5.0))\n",
    "# load pre-trained model before -- you can use this to continously train the model if the memory capacity is not enough # Previous total iterations = 1k\n",
    "# load_weights(meta_model, model_weights_path_metarl_training) \n",
    "# train meta-model\n",
    "meta_rl_training(meta_model)\n",
    "save_weights(meta_model, model_weights_path_metarl_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLER = {}\n",
    "# BER = {}\n",
    "\n",
    "# # Range of SNRs over which the systems are evaluated\n",
    "# ebno_dbs = np.arange(ebno_db_min, # Min SNR for evaluation\n",
    "#                      ebno_db_max, # Max SNR for evaluation\n",
    "#                      0.5) # Step\n",
    "\n",
    "# model_metarl = E2ESystemMetaRLTraining(training=False)\n",
    "# load_weights(model_metarl, model_weights_path_metarl_training)\n",
    "# ber,bler = sim_ber(model_metarl, ebno_dbs, batch_size=128, num_target_block_errors=1000, max_mc_iter=1000)\n",
    "# BLER['autoencoder-metarl'] = bler.numpy()\n",
    "# BER['autoencoder-metarl'] = ber.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYR15EbbagI2",
    "outputId": "99842721-09cd-4607-dfbf-2f49a38f4b71"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Baseline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Range of SNRs over which the systems are evaluated\u001b[39;00m\n\u001b[0;32m      9\u001b[0m ebno_dbs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(ebno_db_min, \u001b[38;5;66;03m# Min SNR for evaluation\u001b[39;00m\n\u001b[0;32m     10\u001b[0m                      ebno_db_max, \u001b[38;5;66;03m# Max SNR for evaluation\u001b[39;00m\n\u001b[0;32m     11\u001b[0m                      \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;66;03m# Step\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m model_baseline \u001b[38;5;241m=\u001b[39m \u001b[43mBaseline\u001b[49m()\n\u001b[0;32m     14\u001b[0m ber,bler \u001b[38;5;241m=\u001b[39m sim_ber(model_baseline, ebno_dbs, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, num_target_block_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, max_mc_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     15\u001b[0m BLER[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbaseline\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m bler\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Baseline' is not defined"
     ]
    }
   ],
   "source": [
    "# Dictionnary storing the results\n",
    "# with open('base_result', 'rb') as f:\n",
    "#     BLER = pickle.load(f)\n",
    "\n",
    "BLER = {}\n",
    "BER = {}\n",
    "\n",
    "# Range of SNRs over which the systems are evaluated\n",
    "ebno_dbs = np.arange(ebno_db_min, # Min SNR for evaluation\n",
    "                     ebno_db_max, # Max SNR for evaluation\n",
    "                     0.5) # Step\n",
    "\n",
    "model_baseline = Baseline()\n",
    "ber,bler = sim_ber(model_baseline, ebno_dbs, batch_size=128, num_target_block_errors=1000, max_mc_iter=1000)\n",
    "BLER['baseline'] = bler.numpy()\n",
    "BER['baseline'] = ber.numpy()\n",
    "\n",
    "\n",
    "model_conventional = E2ESystemConventionalTraining(training=False)\n",
    "load_weights(model_conventional, model_weights_path_conventional_training)\n",
    "ber,bler = sim_ber(model_conventional, ebno_dbs, batch_size=128, num_target_block_errors=1000, max_mc_iter=1000)\n",
    "BLER['autoencoder-conv'] = bler.numpy()\n",
    "BER['autoencoder-conv'] = ber.numpy()\n",
    "\n",
    "model_rl = E2ESystemRLTraining(training=False)\n",
    "load_weights(model_rl, model_weights_path_rl_training)\n",
    "ber,bler = sim_ber(model_rl, ebno_dbs, batch_size=128, num_target_block_errors=1000, max_mc_iter=1000)\n",
    "BLER['autoencoder-rl'] = bler.numpy()\n",
    "BER['autoencoder-rl'] = ber.numpy()\n",
    "\n",
    "model_metarl = E2ESystemMetaRLTraining(training=False)\n",
    "load_weights(model_metarl, model_weights_path_metarl_training)\n",
    "ber,bler = sim_ber(model_metarl, ebno_dbs, batch_size=128, num_target_block_errors=1000, max_mc_iter=1000)\n",
    "BLER['autoencoder-metarl'] = bler.numpy()\n",
    "BER['autoencoder-metarl'] = ber.numpy()\n",
    "\n",
    "with open(results_filename, 'wb') as f:\n",
    "    pickle.dump((ebno_dbs, BLER), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "6xTnt0Gxhl_y",
    "outputId": "e3f1bda3-887e-4aff-ee6a-8acfc782bb1f"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'baseline'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39msemilogy(ebno_dbs, \u001b[43mBLER\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbaseline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo-\u001b[39m\u001b[38;5;124m'\u001b[39m, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC0\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaseline\u001b[39m\u001b[38;5;124m'\u001b[39m)    \n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39msemilogy(ebno_dbs, BLER[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoencoder-conv\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx-.\u001b[39m\u001b[38;5;124m'\u001b[39m, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC1\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoencoder - conventional training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39msemilogy(ebno_dbs, BLER[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoencoder-rl\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo-.\u001b[39m\u001b[38;5;124m'\u001b[39m, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC2\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoencoder - RL-based training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'baseline'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.semilogy(ebno_dbs, BLER['baseline'], 'o-', c=f'C0', label=f'Baseline')    \n",
    "plt.semilogy(ebno_dbs, BLER['autoencoder-conv'], 'x-.', c=f'C1', label=f'Autoencoder - conventional training')\n",
    "plt.semilogy(ebno_dbs, BLER['autoencoder-rl'], 'o-.', c=f'C2', label=f'Autoencoder - RL-based training')\n",
    "plt.semilogy(ebno_dbs, BLER['autoencoder-metarl'], 'o-.', c=f'C3', label=f'Autoencoder - Meta-RL-based training')\n",
    "\n",
    "plt.xlabel(r\"$E_b/N_0$ (dB)\")\n",
    "plt.ylabel(\"BLER\")\n",
    "plt.grid(which=\"both\")\n",
    "plt.ylim((1e-5, 1.0))\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'baseline'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39msemilogy(ebno_dbs, \u001b[43mBER\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbaseline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo-\u001b[39m\u001b[38;5;124m'\u001b[39m, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC0\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaseline\u001b[39m\u001b[38;5;124m'\u001b[39m)    \n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39msemilogy(ebno_dbs, BER[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoencoder-conv\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx-.\u001b[39m\u001b[38;5;124m'\u001b[39m, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC1\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoencoder - conventional training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39msemilogy(ebno_dbs, BER[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoencoder-rl\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo-.\u001b[39m\u001b[38;5;124m'\u001b[39m, c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC2\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoencoder - RL-based training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'baseline'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.semilogy(ebno_dbs, BER['baseline'], 'o-', c=f'C0', label=f'Baseline')    \n",
    "plt.semilogy(ebno_dbs, BER['autoencoder-conv'], 'x-.', c=f'C1', label=f'Autoencoder - conventional training')\n",
    "plt.semilogy(ebno_dbs, BER['autoencoder-rl'], 'o-.', c=f'C2', label=f'Autoencoder - RL-based training')\n",
    "plt.semilogy(ebno_dbs, BER['autoencoder-metarl'], 'o-.', c=f'C3', label=f'Autoencoder - Meta-RL-based training')\n",
    "\n",
    "plt.xlabel(r\"$E_b/N_0$ (dB)\")\n",
    "plt.ylabel(\"BER\")\n",
    "plt.grid(which=\"both\")\n",
    "plt.ylim((1e-8, 1.0))\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
